{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Analysis and Preprocessing\n",
    "## Advanced Fraud Detection System\n",
    "\n",
    "This notebook covers comprehensive data analysis and preprocessing for both e-commerce and credit card fraud detection datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "fraud_data = pd.read_csv('../data/Fraud_Data.csv')\n",
    "ip_data = pd.read_csv('../data/IpAddress_to_Country.csv')\n",
    "credit_card_data = pd.read_csv('../data/creditcard.csv')\n",
    "\n",
    "print(f\"Fraud Data Shape: {fraud_data.shape}\")\n",
    "print(f\"IP Data Shape: {ip_data.shape}\")\n",
    "print(f\"Credit Card Data Shape: {credit_card_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about datasets\n",
    "print(\"=== FRAUD DATA INFO ===\")\n",
    "print(fraud_data.info())\n",
    "print(\"\\n=== FRAUD DATA HEAD ===\")\n",
    "print(fraud_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CREDIT CARD DATA INFO ===\")\n",
    "print(credit_card_data.info())\n",
    "print(\"\\n=== CREDIT CARD DATA HEAD ===\")\n",
    "print(credit_card_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== IP DATA INFO ===\")\n",
    "print(ip_data.info())\n",
    "print(\"\\n=== IP DATA HEAD ===\")\n",
    "print(ip_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in all datasets\n",
    "print(\"=== MISSING VALUES ANALYSIS ===\")\n",
    "print(\"\\nFraud Data Missing Values:\")\n",
    "fraud_missing = fraud_data.isnull().sum()\n",
    "print(fraud_missing[fraud_missing > 0])\n",
    "print(f\"Total missing values: {fraud_missing.sum()}\")\n",
    "\n",
    "print(\"\\nCredit Card Data Missing Values:\")\n",
    "credit_missing = credit_card_data.isnull().sum()\n",
    "print(credit_missing[credit_missing > 0])\n",
    "print(f\"Total missing values: {credit_missing.sum()}\")\n",
    "\n",
    "print(\"\\nIP Data Missing Values:\")\n",
    "ip_missing = ip_data.isnull().sum()\n",
    "print(ip_missing[ip_missing > 0])\n",
    "print(f\"Total missing values: {ip_missing.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values if any exist\n",
    "# For fraud data - fill missing numerical values with median, categorical with mode\n",
    "for col in fraud_data.columns:\n",
    "    if fraud_data[col].isnull().sum() > 0:\n",
    "        if fraud_data[col].dtype in ['int64', 'float64']:\n",
    "            fraud_data[col].fillna(fraud_data[col].median(), inplace=True)\n",
    "        else:\n",
    "            fraud_data[col].fillna(fraud_data[col].mode()[0], inplace=True)\n",
    "\n",
    "# For credit card data - fill with median for numerical columns\n",
    "for col in credit_card_data.columns:\n",
    "    if credit_card_data[col].isnull().sum() > 0:\n",
    "        credit_card_data[col].fillna(credit_card_data[col].median(), inplace=True)\n",
    "\n",
    "print(\"Missing values handled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "print(\"=== DUPLICATE ANALYSIS ===\")\n",
    "print(f\"Fraud data duplicates: {fraud_data.duplicated().sum()}\")\n",
    "print(f\"Credit card data duplicates: {credit_card_data.duplicated().sum()}\")\n",
    "print(f\"IP data duplicates: {ip_data.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates\n",
    "fraud_data_clean = fraud_data.drop_duplicates()\n",
    "credit_card_data_clean = credit_card_data.drop_duplicates()\n",
    "ip_data_clean = ip_data.drop_duplicates()\n",
    "\n",
    "print(f\"\\nAfter removing duplicates:\")\n",
    "print(f\"Fraud data shape: {fraud_data_clean.shape}\")\n",
    "print(f\"Credit card data shape: {credit_card_data_clean.shape}\")\n",
    "print(f\"IP data shape: {ip_data_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct data types\n",
    "print(\"=== DATA TYPE CORRECTION ===\")\n",
    "\n",
    "# Convert datetime columns in fraud data\n",
    "fraud_data_clean['signup_time'] = pd.to_datetime(fraud_data_clean['signup_time'])\n",
    "fraud_data_clean['purchase_time'] = pd.to_datetime(fraud_data_clean['purchase_time'])\n",
    "\n",
    "# Ensure IP addresses are properly formatted\n",
    "fraud_data_clean['ip_address'] = fraud_data_clean['ip_address'].astype(float)\n",
    "\n",
    "print(\"Data types corrected successfully!\")\n",
    "print(\"\\nFraud data types:\")\n",
    "print(fraud_data_clean.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis - Fraud Data\n",
    "print(\"=== UNIVARIATE ANALYSIS - FRAUD DATA ===\")\n",
    "\n",
    "# Target variable distribution\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "fraud_counts = fraud_data_clean['class'].value_counts()\n",
    "plt.pie(fraud_counts.values, labels=['Non-Fraud', 'Fraud'], autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Fraud vs Non-Fraud Distribution')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(fraud_data_clean['purchase_value'], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Purchase Value Distribution')\n",
    "plt.xlabel('Purchase Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(fraud_data_clean['age'], bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "source_counts = fraud_data_clean['source'].value_counts()\n",
    "plt.bar(source_counts.index, source_counts.values)\n",
    "plt.title('Traffic Source Distribution')\n",
    "plt.xlabel('Source')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "browser_counts = fraud_data_clean['browser'].value_counts()\n",
    "plt.bar(browser_counts.index, browser_counts.values)\n",
    "plt.title('Browser Distribution')\n",
    "plt.xlabel('Browser')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "sex_counts = fraud_data_clean['sex'].value_counts()\n",
    "plt.bar(sex_counts.index, sex_counts.values)\n",
    "plt.title('Gender Distribution')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis - Credit Card Data\n",
    "print(\"=== UNIVARIATE ANALYSIS - CREDIT CARD DATA ===\")\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "credit_counts = credit_card_data_clean['Class'].value_counts()\n",
    "plt.pie(credit_counts.values, labels=['Non-Fraud', 'Fraud'], autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Credit Card Fraud Distribution')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(credit_card_data_clean['Amount'], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Transaction Amount Distribution')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(credit_card_data_clean['Time'], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Transaction Time Distribution')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show some V features\n",
    "for i, v_col in enumerate(['V1', 'V2', 'V3'], 4):\n",
    "    plt.subplot(2, 3, i)\n",
    "    plt.hist(credit_card_data_clean[v_col], bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.title(f'{v_col} Distribution')\n",
    "    plt.xlabel(v_col)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis - Fraud Data\n",
    "print(\"=== BIVARIATE ANALYSIS - FRAUD DATA ===\")\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Purchase value vs Fraud\n",
    "plt.subplot(3, 3, 1)\n",
    "fraud_data_clean.boxplot(column='purchase_value', by='class', ax=plt.gca())\n",
    "plt.title('Purchase Value by Fraud Status')\n",
    "plt.suptitle('')\n",
    "\n",
    "# Age vs Fraud\n",
    "plt.subplot(3, 3, 2)\n",
    "fraud_data_clean.boxplot(column='age', by='class', ax=plt.gca())\n",
    "plt.title('Age by Fraud Status')\n",
    "plt.suptitle('')\n",
    "\n",
    "# Source vs Fraud\n",
    "plt.subplot(3, 3, 3)\n",
    "source_fraud = pd.crosstab(fraud_data_clean['source'], fraud_data_clean['class'], normalize='index')\n",
    "source_fraud.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Fraud Rate by Source')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(['Non-Fraud', 'Fraud'])\n",
    "\n",
    "# Browser vs Fraud\n",
    "plt.subplot(3, 3, 4)\n",
    "browser_fraud = pd.crosstab(fraud_data_clean['browser'], fraud_data_clean['class'], normalize='index')\n",
    "browser_fraud.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Fraud Rate by Browser')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(['Non-Fraud', 'Fraud'])\n",
    "\n",
    "# Gender vs Fraud\n",
    "plt.subplot(3, 3, 5)\n",
    "sex_fraud = pd.crosstab(fraud_data_clean['sex'], fraud_data_clean['class'], normalize='index')\n",
    "sex_fraud.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Fraud Rate by Gender')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(['Non-Fraud', 'Fraud'])\n",
    "\n",
    "# Correlation heatmap for numerical features\n",
    "plt.subplot(3, 3, 6)\n",
    "numerical_cols = ['purchase_value', 'age', 'class']\n",
    "correlation_matrix = fraud_data_clean[numerical_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=plt.gca())\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis - Credit Card Data\n",
    "print(\"=== BIVARIATE ANALYSIS - CREDIT CARD DATA ===\")\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Amount vs Fraud\n",
    "plt.subplot(2, 3, 1)\n",
    "credit_card_data_clean.boxplot(column='Amount', by='Class', ax=plt.gca())\n",
    "plt.title('Transaction Amount by Fraud Status')\n",
    "plt.suptitle('')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Time vs Fraud\n",
    "plt.subplot(2, 3, 2)\n",
    "fraud_transactions = credit_card_data_clean[credit_card_data_clean['Class'] == 1]['Time']\n",
    "normal_transactions = credit_card_data_clean[credit_card_data_clean['Class'] == 0]['Time']\n",
    "plt.hist([normal_transactions, fraud_transactions], bins=50, alpha=0.7, \n",
    "         label=['Normal', 'Fraud'], color=['blue', 'red'])\n",
    "plt.title('Transaction Time Distribution by Class')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "# V1 vs Fraud\n",
    "plt.subplot(2, 3, 3)\n",
    "credit_card_data_clean.boxplot(column='V1', by='Class', ax=plt.gca())\n",
    "plt.title('V1 by Fraud Status')\n",
    "plt.suptitle('')\n",
    "\n",
    "# V2 vs Fraud\n",
    "plt.subplot(2, 3, 4)\n",
    "credit_card_data_clean.boxplot(column='V2', by='Class', ax=plt.gca())\n",
    "plt.title('V2 by Fraud Status')\n",
    "plt.suptitle('')\n",
    "\n",
    "# Correlation heatmap for selected V features\n",
    "plt.subplot(2, 3, 5)\n",
    "v_features = ['V1', 'V2', 'V3', 'V4', 'V5', 'Amount', 'Class']\n",
    "correlation_matrix = credit_card_data_clean[v_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=plt.gca())\n",
    "plt.title('Correlation Matrix (Selected Features)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge Datasets for Geolocation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IP addresses to integer format for merging\n",
    "print(\"=== GEOLOCATION ANALYSIS ===\")\n",
    "\n",
    "def map_ip_to_country(ip_address, ip_data):\n",
    "    \"\"\"Map IP address to country using IP ranges\"\"\"\n",
    "    for _, row in ip_data.iterrows():\n",
    "        if row['lower_bound_ip_address'] <= ip_address <= row['upper_bound_ip_address']:\n",
    "            return row['country']\n",
    "    return 'Unknown'\n",
    "\n",
    "# Sample a subset for demonstration (mapping all IPs would be computationally expensive)\n",
    "sample_fraud_data = fraud_data_clean.sample(n=10000, random_state=42)\n",
    "\n",
    "print(\"Mapping IP addresses to countries (sample of 10,000 records)...\")\n",
    "sample_fraud_data['country'] = sample_fraud_data['ip_address'].apply(\n",
    "    lambda x: map_ip_to_country(x, ip_data_clean)\n",
    ")\n",
    "\n",
    "print(\"\\nCountry distribution in sample:\")\n",
    "print(sample_fraud_data['country'].value_counts().head(10))\n",
    "\n",
    "# Analyze fraud by country\n",
    "country_fraud = pd.crosstab(sample_fraud_data['country'], sample_fraud_data['class'], normalize='index')\n",
    "print(\"\\nFraud rate by country (top 10):\")\n",
    "fraud_rates = country_fraud[1].sort_values(ascending=False).head(10)\n",
    "print(fraud_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize geolocation analysis\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "country_counts = sample_fraud_data['country'].value_counts().head(10)\n",
    "plt.bar(country_counts.index, country_counts.values)\n",
    "plt.title('Top 10 Countries by Transaction Volume')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "fraud_rates_plot = country_fraud[1].sort_values(ascending=False).head(10)\n",
    "plt.bar(fraud_rates_plot.index, fraud_rates_plot.values)\n",
    "plt.title('Top 10 Countries by Fraud Rate')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Fraud Rate')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "# Fraud distribution by country (absolute numbers)\n",
    "country_fraud_abs = pd.crosstab(sample_fraud_data['country'], sample_fraud_data['class'])\n",
    "fraud_counts_by_country = country_fraud_abs[1].sort_values(ascending=False).head(10)\n",
    "plt.bar(fraud_counts_by_country.index, fraud_counts_by_country.values, color='red', alpha=0.7)\n",
    "plt.title('Top 10 Countries by Fraud Count')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Number of Fraud Cases')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for Fraud Data\n",
    "print(\"=== FEATURE ENGINEERING - FRAUD DATA ===\")\n",
    "\n",
    "# Time-based features\n",
    "fraud_data_clean['hour_of_day'] = fraud_data_clean['purchase_time'].dt.hour\n",
    "fraud_data_clean['day_of_week'] = fraud_data_clean['purchase_time'].dt.dayofweek\n",
    "fraud_data_clean['month'] = fraud_data_clean['purchase_time'].dt.month\n",
    "fraud_data_clean['is_weekend'] = fraud_data_clean['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Time difference between signup and purchase\n",
    "fraud_data_clean['time_diff_hours'] = (\n",
    "    fraud_data_clean['purchase_time'] - fraud_data_clean['signup_time']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "# Transaction frequency and velocity features\n",
    "user_stats = fraud_data_clean.groupby('user_id').agg({\n",
    "    'purchase_value': ['count', 'sum', 'mean', 'std'],\n",
    "    'purchase_time': ['min', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "user_stats.columns = ['user_id', 'transaction_count', 'total_spent', 'avg_transaction', 'std_transaction',\n",
    "                     'first_transaction', 'last_transaction']\n",
    "\n",
    "# Calculate transaction velocity (transactions per day)\n",
    "user_stats['transaction_span_days'] = (\n",
    "    user_stats['last_transaction'] - user_stats['first_transaction']\n",
    ").dt.total_seconds() / (24 * 3600)\n",
    "user_stats['transaction_span_days'] = user_stats['transaction_span_days'].fillna(1)  # Handle same-day transactions\n",
    "user_stats['transactions_per_day'] = user_stats['transaction_count'] / user_stats['transaction_span_days']\n",
    "\n",
    "# Merge back to main dataset\n",
    "fraud_data_engineered = fraud_data_clean.merge(user_stats[['user_id', 'transaction_count', 'transactions_per_day']], \n",
    "                                              on='user_id', how='left')\n",
    "\n",
    "print(f\"Original features: {fraud_data_clean.shape[1]}\")\n",
    "print(f\"After feature engineering: {fraud_data_engineered.shape[1]}\")\n",
    "print(\"\\nNew features created:\")\n",
    "new_features = ['hour_of_day', 'day_of_week', 'month', 'is_weekend', 'time_diff_hours', \n",
    "               'transaction_count', 'transactions_per_day']\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for Credit Card Data\n",
    "print(\"=== FEATURE ENGINEERING - CREDIT CARD DATA ===\")\n",
    "\n",
    "# Time-based features\n",
    "credit_card_data_clean['hour'] = (credit_card_data_clean['Time'] / 3600) % 24\n",
    "credit_card_data_clean['day'] = (credit_card_data_clean['Time'] / (24 * 3600)) % 7\n",
    "\n",
    "# Amount-based features\n",
    "credit_card_data_clean['amount_log'] = np.log1p(credit_card_data_clean['Amount'])\n",
    "credit_card_data_clean['amount_sqrt'] = np.sqrt(credit_card_data_clean['Amount'])\n",
    "\n",
    "# Statistical features for V columns\n",
    "v_columns = [col for col in credit_card_data_clean.columns if col.startswith('V')]\n",
    "credit_card_data_clean['v_mean'] = credit_card_data_clean[v_columns].mean(axis=1)\n",
    "credit_card_data_clean['v_std'] = credit_card_data_clean[v_columns].std(axis=1)\n",
    "credit_card_data_clean['v_sum'] = credit_card_data_clean[v_columns].sum(axis=1)\n",
    "\n",
    "print(f\"Original features: {len(v_columns) + 3}\")\n",
    "print(f\"After feature engineering: {credit_card_data_clean.shape[1]}\")\n",
    "print(\"\\nNew features created:\")\n",
    "new_credit_features = ['hour', 'day', 'amount_log', 'amount_sqrt', 'v_mean', 'v_std', 'v_sum']\n",
    "print(new_credit_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize new features\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Fraud data features\n",
    "plt.subplot(3, 4, 1)\n",
    "hour_fraud = pd.crosstab(fraud_data_engineered['hour_of_day'], fraud_data_engineered['class'], normalize='index')\n",
    "hour_fraud[1].plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Fraud Rate by Hour of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Fraud Rate')\n",
    "\n",
    "plt.subplot(3, 4, 2)\n",
    "dow_fraud = pd.crosstab(fraud_data_engineered['day_of_week'], fraud_data_engineered['class'], normalize='index')\n",
    "dow_fraud[1].plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Fraud Rate by Day of Week')\n",
    "plt.xlabel('Day of Week (0=Monday)')\n",
    "plt.ylabel('Fraud Rate')\n",
    "\n",
    "plt.subplot(3, 4, 3)\n",
    "plt.hist(fraud_data_engineered['time_diff_hours'], bins=50, alpha=0.7)\n",
    "plt.title('Time Difference (Signup to Purchase)')\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(3, 4, 4)\n",
    "plt.hist(fraud_data_engineered['transactions_per_day'], bins=50, alpha=0.7)\n",
    "plt.title('Transactions Per Day')\n",
    "plt.xlabel('Transactions/Day')\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Credit card data features\n",
    "plt.subplot(3, 4, 5)\n",
    "credit_hour_fraud = credit_card_data_clean.groupby('hour')['Class'].mean()\n",
    "credit_hour_fraud.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Credit Card Fraud Rate by Hour')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Fraud Rate')\n",
    "\n",
    "plt.subplot(3, 4, 6)\n",
    "plt.hist(credit_card_data_clean['amount_log'], bins=50, alpha=0.7)\n",
    "plt.title('Log-transformed Amount')\n",
    "plt.xlabel('Log(Amount + 1)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(3, 4, 7)\n",
    "plt.hist(credit_card_data_clean['v_mean'], bins=50, alpha=0.7)\n",
    "plt.title('Mean of V Features')\n",
    "plt.xlabel('V Mean')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(3, 4, 8)\n",
    "plt.hist(credit_card_data_clean['v_std'], bins=50, alpha=0.7)\n",
    "plt.title('Standard Deviation of V Features')\n",
    "plt.xlabel('V Std')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== CATEGORICAL ENCODING ===\")\n",
    "\n",
    "# For fraud data\n",
    "fraud_data_encoded = fraud_data_engineered.copy()\n",
    "\n",
    "# Label encoding for ordinal-like categories\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_cols = ['source', 'browser', 'sex']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    fraud_data_encoded[f'{col}_encoded'] = label_encoder.fit_transform(fraud_data_encoded[col])\n",
    "\n",
    "print(\"Categorical features encoded for fraud data:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"{col}: {fraud_data_encoded[col].unique()} -> {fraud_data_encoded[f'{col}_encoded'].unique()}\")\n",
    "\n",
    "# One-hot encoding for nominal categories (alternative approach)\n",
    "fraud_data_onehot = pd.get_dummies(fraud_data_engineered, \n",
    "                                  columns=['source', 'browser', 'sex'], \n",
    "                                  prefix=['source', 'browser', 'sex'])\n",
    "\n",
    "print(f\"\\nOriginal shape: {fraud_data_engineered.shape}\")\n",
    "print(f\"After label encoding: {fraud_data_encoded.shape}\")\n",
    "print(f\"After one-hot encoding: {fraud_data_onehot.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Normalization and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization and Scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "print(\"=== NORMALIZATION AND SCALING ===\")\n",
    "\n",
    "# Prepare fraud data for scaling\n",
    "# Select numerical features for scaling\n",
    "fraud_numerical_features = ['purchase_value', 'age', 'hour_of_day', 'day_of_week', 'month', \n",
    "                           'is_weekend', 'time_diff_hours', 'transaction_count', 'transactions_per_day']\n",
    "\n",
    "# Remove any features with NaN values\n",
    "fraud_data_for_scaling = fraud_data_encoded[fraud_numerical_features].fillna(0)\n",
    "\n",
    "# Apply different scaling methods\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "\n",
    "fraud_scaled_data = {}\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    fraud_scaled_data[scaler_name] = pd.DataFrame(\n",
    "        scaler.fit_transform(fraud_data_for_scaling),\n",
    "        columns=fraud_numerical_features,\n",
    "        index=fraud_data_for_scaling.index\n",
    "    )\n",
    "\n",
    "print(\"Scaling completed for fraud data\")\n",
    "\n",
    "# Prepare credit card data for scaling\n",
    "credit_numerical_features = ['Time', 'Amount'] + v_columns + ['hour', 'day', 'amount_log', 'amount_sqrt', 'v_mean', 'v_std', 'v_sum']\n",
    "credit_data_for_scaling = credit_card_data_clean[credit_numerical_features].fillna(0)\n",
    "\n",
    "credit_scaled_data = {}\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    credit_scaled_data[scaler_name] = pd.DataFrame(\n",
    "        scaler.fit_transform(credit_data_for_scaling),\n",
    "        columns=credit_numerical_features,\n",
    "        index=credit_data_for_scaling.index\n",
    "    )\n",
    "\n",
    "print(\"Scaling completed for credit card data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of different scaling methods\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Original vs scaled distributions for fraud data\n",
    "feature_to_plot = 'purchase_value'\n",
    "\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.hist(fraud_data_for_scaling[feature_to_plot], bins=50, alpha=0.7)\n",
    "plt.title(f'Original {feature_to_plot}')\n",
    "plt.xlabel(feature_to_plot)\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "for i, (scaler_name, scaled_data) in enumerate(fraud_scaled_data.items(), 2):\n",
    "    plt.subplot(2, 4, i)\n",
    "    plt.hist(scaled_data[feature_to_plot], bins=50, alpha=0.7)\n",
    "    plt.title(f'{scaler_name} - {feature_to_plot}')\n",
    "    plt.xlabel(f'Scaled {feature_to_plot}')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "# Original vs scaled distributions for credit card data\n",
    "credit_feature_to_plot = 'Amount'\n",
    "\n",
    "plt.subplot(2, 4, 5)\n",
    "plt.hist(credit_data_for_scaling[credit_feature_to_plot], bins=50, alpha=0.7)\n",
    "plt.title(f'Original {credit_feature_to_plot}')\n",
    "plt.xlabel(credit_feature_to_plot)\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log')\n",
    "\n",
    "for i, (scaler_name, scaled_data) in enumerate(credit_scaled_data.items(), 6):\n",
    "    plt.subplot(2, 4, i)\n",
    "    plt.hist(scaled_data[credit_feature_to_plot], bins=50, alpha=0.7)\n",
    "    plt.title(f'{scaler_name} - {credit_feature_to_plot}')\n",
    "    plt.xlabel(f'Scaled {credit_feature_to_plot}')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Data Preparation and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final datasets for modeling\n",
    "print(\"=== FINAL DATA PREPARATION ===\")\n",
    "\n",
    "# Fraud data - combine scaled features with categorical encoded features\n",
    "fraud_final = fraud_scaled_data['StandardScaler'].copy()\n",
    "fraud_final['class'] = fraud_data_encoded['class']\n",
    "fraud_final['source_encoded'] = fraud_data_encoded['source_encoded']\n",
    "fraud_final['browser_encoded'] = fraud_data_encoded['browser_encoded']\n",
    "fraud_final['sex_encoded'] = fraud_data_encoded['sex_encoded']\n",
    "\n",
    "# Credit card data - use scaled features\n",
    "credit_final = credit_scaled_data['StandardScaler'].copy()\n",
    "credit_final['Class'] = credit_card_data_clean['Class']\n",
    "\n",
    "print(f\"Final fraud data shape: {fraud_final.shape}\")\n",
    "print(f\"Final credit card data shape: {credit_final.shape}\")\n",
    "\n",
    "# Save processed datasets\n",
    "fraud_final.to_csv('../data/fraud_data_processed.csv', index=False)\n",
    "credit_final.to_csv('../data/credit_card_processed.csv', index=False)\n",
    "\n",
    "print(\"\\nProcessed datasets saved successfully!\")\n",
    "print(\"Files saved:\")\n",
    "print(\"- ../data/fraud_data_processed.csv\")\n",
    "print(\"- ../data/credit_card_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of final datasets\n",
    "print(\"=== FINAL DATASET SUMMARY ===\")\n",
    "\n",
    "print(\"\\nFraud Data Summary:\")\n",
    "print(f\"Total samples: {len(fraud_final)}\")\n",
    "print(f\"Features: {fraud_final.shape[1] - 1}\")\n",
    "print(f\"Fraud cases: {fraud_final['class'].sum()} ({fraud_final['class'].mean()*100:.2f}%)\")\n",
    "print(f\"Non-fraud cases: {(fraud_final['class'] == 0).sum()} ({(1-fraud_final['class'].mean())*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nCredit Card Data Summary:\")\n",
    "print(f\"Total samples: {len(credit_final)}\")\n",
    "print(f\"Features: {credit_final.shape[1] - 1}\")\n",
    "print(f\"Fraud cases: {credit_final['Class'].sum()} ({credit_final['Class'].mean()*100:.2f}%)\")\n",
    "print(f\"Non-fraud cases: {(credit_final['Class'] == 0).sum()} ({(1-credit_final['Class'].mean())*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n=== TASK 1 COMPLETED SUCCESSFULLY ===\")\n",
    "print(\"All data analysis and preprocessing steps have been completed:\")\n",
    "print(\"✅ Missing values handled\")\n",
    "print(\"✅ Data cleaning completed\")\n",
    "print(\"✅ Exploratory data analysis performed\")\n",
    "print(\"✅ Datasets merged for geolocation analysis\")\n",
    "print(\"✅ Feature engineering completed\")\n",
    "print(\"✅ Categorical features encoded\")\n",
    "print(\"✅ Normalization and scaling applied\")\n",
    "print(\"✅ Final datasets prepared and saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}