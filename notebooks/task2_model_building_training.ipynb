{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Model Building and Training\n",
    "## Advanced Fraud Detection System\n",
    "\n",
    "This notebook covers comprehensive model building and training with MLOps practices using MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, f1_score, accuracy_score\n",
    ")\n",
    "\n",
    "# Traditional ML models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Deep Learning models\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# MLOps\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.tensorflow\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed datasets\n",
    "print(\"Loading processed datasets...\")\n",
    "try:\n",
    "    fraud_data = pd.read_csv('../data/fraud_data_processed.csv')\n",
    "    credit_data = pd.read_csv('../data/credit_card_processed.csv')\n",
    "    print(f\"Fraud data shape: {fraud_data.shape}\")\n",
    "    print(f\"Credit card data shape: {credit_data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Processed datasets not found. Loading original datasets...\")\n",
    "    fraud_data = pd.read_csv('../data/Fraud_Data.csv')\n",
    "    credit_data = pd.read_csv('../data/creditcard.csv')\n",
    "    print(\"Please run Task 1 notebook first for optimal results.\")\n",
    "\n",
    "print(\"\\nDataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature and Target Separation\n",
    "print(\"=== FEATURE AND TARGET SEPARATION ===\")\n",
    "\n",
    "# Fraud Data\n",
    "if 'class' in fraud_data.columns:\n",
    "    X_fraud = fraud_data.drop(columns=['class'])\n",
    "    y_fraud = fraud_data['class']\n",
    "else:\n",
    "    # Fallback for original data\n",
    "    fraud_data_processed = fraud_data.copy()\n",
    "    # Basic preprocessing\n",
    "    fraud_data_processed['signup_time'] = pd.to_datetime(fraud_data_processed['signup_time'])\n",
    "    fraud_data_processed['purchase_time'] = pd.to_datetime(fraud_data_processed['purchase_time'])\n",
    "    fraud_data_processed['hour_of_day'] = fraud_data_processed['purchase_time'].dt.hour\n",
    "    fraud_data_processed['day_of_week'] = fraud_data_processed['purchase_time'].dt.dayofweek\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    for col in ['source', 'browser', 'sex']:\n",
    "        fraud_data_processed[f'{col}_encoded'] = le.fit_transform(fraud_data_processed[col])\n",
    "    \n",
    "    # Select features\n",
    "    feature_cols = ['purchase_value', 'age', 'hour_of_day', 'day_of_week', \n",
    "                   'source_encoded', 'browser_encoded', 'sex_encoded']\n",
    "    X_fraud = fraud_data_processed[feature_cols]\n",
    "    y_fraud = fraud_data_processed['class']\n",
    "\n",
    "# Credit Card Data\n",
    "if 'Class' in credit_data.columns:\n",
    "    X_credit = credit_data.drop(columns=['Class'])\n",
    "    y_credit = credit_data['Class']\n",
    "else:\n",
    "    print(\"Error: Class column not found in credit card data\")\n",
    "\n",
    "print(f\"Fraud features shape: {X_fraud.shape}\")\n",
    "print(f\"Fraud target distribution: {y_fraud.value_counts()}\")\n",
    "print(f\"Credit features shape: {X_credit.shape}\")\n",
    "print(f\"Credit target distribution: {y_credit.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "print(\"=== TRAIN-TEST SPLIT ===\")\n",
    "\n",
    "# Fraud Data Split\n",
    "X_fraud_train, X_fraud_test, y_fraud_train, y_fraud_test = train_test_split(\n",
    "    X_fraud, y_fraud, test_size=0.2, random_state=42, stratify=y_fraud\n",
    ")\n",
    "\n",
    "# Credit Card Data Split\n",
    "X_credit_train, X_credit_test, y_credit_train, y_credit_test = train_test_split(\n",
    "    X_credit, y_credit, test_size=0.2, random_state=42, stratify=y_credit\n",
    ")\n",
    "\n",
    "print(f\"Fraud training set: {X_fraud_train.shape}\")\n",
    "print(f\"Fraud test set: {X_fraud_test.shape}\")\n",
    "print(f\"Credit training set: {X_credit_train.shape}\")\n",
    "print(f\"Credit test set: {X_credit_test.shape}\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "print(f\"\\nFraud training target distribution: {y_fraud_train.value_counts()}\")\n",
    "print(f\"Credit training target distribution: {y_credit_train.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "print(\"=== FEATURE SCALING ===\")\n",
    "\n",
    "# Scale features for better model performance\n",
    "scaler_fraud = StandardScaler()\n",
    "scaler_credit = StandardScaler()\n",
    "\n",
    "X_fraud_train_scaled = scaler_fraud.fit_transform(X_fraud_train)\n",
    "X_fraud_test_scaled = scaler_fraud.transform(X_fraud_test)\n",
    "\n",
    "X_credit_train_scaled = scaler_credit.fit_transform(X_credit_train)\n",
    "X_credit_test_scaled = scaler_credit.transform(X_credit_test)\n",
    "\n",
    "print(\"Feature scaling completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLOps Setup with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLflow\n",
    "print(\"=== MLFLOW SETUP ===\")\n",
    "\n",
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "\n",
    "# Create or set experiment\n",
    "experiment_name = \"fraud_detection_advanced\"\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "except mlflow.exceptions.MlflowException:\n",
    "    experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"MLflow experiment set: {experiment_name}\")\n",
    "print(f\"Experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow logging function\n",
    "def log_model_performance(model, model_name, X_train, X_test, y_train, y_test, dataset_name):\n",
    "    \"\"\"Log model performance metrics to MLflow\"\"\"\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_{dataset_name}\"):\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"dataset\", dataset_name)\n",
    "        mlflow.log_param(\"train_size\", len(X_train))\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "        mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        \n",
    "        # Log model\n",
    "        if hasattr(model, 'fit'):  # sklearn models\n",
    "            mlflow.sklearn.log_model(model, f\"{model_name}_{dataset_name}\")\n",
    "        \n",
    "        print(f\"{model_name} ({dataset_name}):\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        return model, {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traditional Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define traditional ML models\n",
    "print(\"=== TRADITIONAL ML MODELS ===\")\n",
    "\n",
    "traditional_models = {\n",
    "    'Logistic_Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision_Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "    'Random_Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient_Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "    'MLP_Classifier': MLPClassifier(random_state=42, max_iter=500, hidden_layer_sizes=(100, 50))\n",
    "}\n",
    "\n",
    "# Store results\n",
    "fraud_results = {}\n",
    "credit_results = {}\n",
    "\n",
    "print(\"Training traditional ML models...\")\n",
    "print(\"\\n=== FRAUD DATA RESULTS ===\")\n",
    "for model_name, model in traditional_models.items():\n",
    "    trained_model, metrics = log_model_performance(\n",
    "        model, model_name, X_fraud_train_scaled, X_fraud_test_scaled, \n",
    "        y_fraud_train, y_fraud_test, \"Fraud\"\n",
    "    )\n",
    "    fraud_results[model_name] = {\n",
    "        'model': trained_model,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "\n",
    "print(\"\\n=== CREDIT CARD DATA RESULTS ===\")\n",
    "for model_name, model in traditional_models.items():\n",
    "    # Create new instance to avoid sklearn warnings\n",
    "    if model_name == 'Logistic_Regression':\n",
    "        model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    elif model_name == 'Decision_Tree':\n",
    "        model = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "    elif model_name == 'Random_Forest':\n",
    "        model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "    elif model_name == 'Gradient_Boosting':\n",
    "        model = GradientBoostingClassifier(random_state=42, n_estimators=100)\n",
    "    elif model_name == 'MLP_Classifier':\n",
    "        model = MLPClassifier(random_state=42, max_iter=500, hidden_layer_sizes=(100, 50))\n",
    "    \n",
    "    trained_model, metrics = log_model_performance(\n",
    "        model, model_name, X_credit_train_scaled, X_credit_test_scaled, \n",
    "        y_credit_train, y_credit_test, \"Credit\"\n",
    "    )\n",
    "    credit_results[model_name] = {\n",
    "        'model': trained_model,\n",
    "        'metrics': metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Models\n",
    "print(\"=== DEEP LEARNING MODELS ===\")\n",
    "\n",
    "def create_mlp_model(input_dim):\n",
    "    \"\"\"Create Multi-Layer Perceptron model\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                 loss='binary_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_cnn_model(input_dim):\n",
    "    \"\"\"Create 1D CNN model for tabular data\"\"\"\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.Reshape((input_dim, 1), input_shape=(input_dim,)),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                 loss='binary_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(input_dim):\n",
    "    \"\"\"Create LSTM model for sequence-like patterns in tabular data\"\"\"\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.Reshape((input_dim, 1), input_shape=(input_dim,)),\n",
    "        LSTM(50, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        LSTM(25),\n",
    "        Dropout(0.3),\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                 loss='binary_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Deep Learning Models\n",
    "def train_deep_model(model_func, model_name, X_train, X_test, y_train, y_test, dataset_name):\n",
    "    \"\"\"Train and evaluate deep learning model\"\"\"\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_{dataset_name}\"):\n",
    "        # Create model\n",
    "        model = model_func(X_train.shape[1])\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_proba = model.predict(X_test).flatten()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"dataset\", dataset_name)\n",
    "        mlflow.log_param(\"epochs\", len(history.history['loss']))\n",
    "        mlflow.log_param(\"batch_size\", 32)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        mlflow.log_metric(\"final_train_loss\", history.history['loss'][-1])\n",
    "        mlflow.log_metric(\"final_val_loss\", history.history['val_loss'][-1])\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.tensorflow.log_model(model, f\"{model_name}_{dataset_name}\")\n",
    "        \n",
    "        print(f\"{model_name} ({dataset_name}):\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "        print(f\"  Training epochs: {len(history.history['loss'])}\")\n",
    "        print()\n",
    "        \n",
    "        return model, {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba,\n",
    "            'history': history\n",
    "        }\n",
    "\n",
    "# Train deep learning models\n",
    "deep_models = {\n",
    "    'MLP': create_mlp_model,\n",
    "    'CNN': create_cnn_model,\n",
    "    'LSTM': create_lstm_model\n",
    "}\n",
    "\n",
    "print(\"Training deep learning models...\")\n",
    "print(\"\\n=== FRAUD DATA - DEEP LEARNING RESULTS ===\")\n",
    "for model_name, model_func in deep_models.items():\n",
    "    trained_model, metrics = train_deep_model(\n",
    "        model_func, model_name, X_fraud_train_scaled, X_fraud_test_scaled, \n",
    "        y_fraud_train, y_fraud_test, \"Fraud\"\n",
    "    )\n",
    "    fraud_results[f\"Deep_{model_name}\"] = {\n",
    "        'model': trained_model,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "\n",
    "print(\"\\n=== CREDIT CARD DATA - DEEP LEARNING RESULTS ===\")\n",
    "for model_name, model_func in deep_models.items():\n",
    "    trained_model, metrics = train_deep_model(\n",
    "        model_func, model_name, X_credit_train_scaled, X_credit_test_scaled, \n",
    "        y_credit_train, y_credit_test, \"Credit\"\n",
    "    )\n",
    "    credit_results[f\"Deep_{model_name}\"] = {\n",
    "        'model': trained_model,\n",
    "        'metrics': metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "\n",
    "def create_comparison_df(results, dataset_name):\n",
    "    \"\"\"Create comparison dataframe for model results\"\"\"\n",
    "    comparison_data = []\n",
    "    for model_name, result in results.items():\n",
    "        metrics = result['metrics']\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Dataset': dataset_name,\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'F1_Score': metrics['f1_score'],\n",
    "            'ROC_AUC': metrics['roc_auc']\n",
    "        })\n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "# Create comparison dataframes\n",
    "fraud_comparison = create_comparison_df(fraud_results, 'Fraud')\n",
    "credit_comparison = create_comparison_df(credit_results, 'Credit')\n",
    "all_comparison = pd.concat([fraud_comparison, credit_comparison], ignore_index=True)\n",
    "\n",
    "print(\"\\nFRAUD DATA MODEL COMPARISON:\")\n",
    "print(fraud_comparison.sort_values('ROC_AUC', ascending=False))\n",
    "\n",
    "print(\"\\nCREDIT CARD DATA MODEL COMPARISON:\")\n",
    "print(credit_comparison.sort_values('ROC_AUC', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# ROC AUC comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "fraud_comparison_sorted = fraud_comparison.sort_values('ROC_AUC', ascending=True)\n",
    "plt.barh(fraud_comparison_sorted['Model'], fraud_comparison_sorted['ROC_AUC'])\n",
    "plt.title('Fraud Data - ROC AUC Comparison')\n",
    "plt.xlabel('ROC AUC Score')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "fraud_comparison_sorted = fraud_comparison.sort_values('F1_Score', ascending=True)\n",
    "plt.barh(fraud_comparison_sorted['Model'], fraud_comparison_sorted['F1_Score'])\n",
    "plt.title('Fraud Data - F1 Score Comparison')\n",
    "plt.xlabel('F1 Score')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "fraud_comparison_sorted = fraud_comparison.sort_values('Accuracy', ascending=True)\n",
    "plt.barh(fraud_comparison_sorted['Model'], fraud_comparison_sorted['Accuracy'])\n",
    "plt.title('Fraud Data - Accuracy Comparison')\n",
    "plt.xlabel('Accuracy')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "credit_comparison_sorted = credit_comparison.sort_values('ROC_AUC', ascending=True)\n",
    "plt.barh(credit_comparison_sorted['Model'], credit_comparison_sorted['ROC_AUC'])\n",
    "plt.title('Credit Data - ROC AUC Comparison')\n",
    "plt.xlabel('ROC AUC Score')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "credit_comparison_sorted = credit_comparison.sort_values('F1_Score', ascending=True)\n",
    "plt.barh(credit_comparison_sorted['Model'], credit_comparison_sorted['F1_Score'])\n",
    "plt.title('Credit Data - F1 Score Comparison')\n",
    "plt.xlabel('F1 Score')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "credit_comparison_sorted = credit_comparison.sort_values('Accuracy', ascending=True)\n",
    "plt.barh(credit_comparison_sorted['Model'], credit_comparison_sorted['Accuracy'])\n",
    "plt.title('Credit Data - Accuracy Comparison')\n",
    "plt.xlabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves for best models\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Get best models based on ROC AUC\n",
    "best_fraud_model = fraud_comparison.loc[fraud_comparison['ROC_AUC'].idxmax(), 'Model']\n",
    "best_credit_model = credit_comparison.loc[credit_comparison['ROC_AUC'].idxmax(), 'Model']\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "# ROC curve for fraud data\n",
    "for model_name in fraud_comparison['Model'].head(3):  # Top 3 models\n",
    "    if model_name in fraud_results:\n",
    "        y_proba = fraud_results[model_name]['metrics']['probabilities']\n",
    "        fpr, tpr, _ = roc_curve(y_fraud_test, y_proba)\n",
    "        auc_score = fraud_results[model_name]['metrics']['roc_auc']\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Fraud Data')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "# ROC curve for credit data\n",
    "for model_name in credit_comparison['Model'].head(3):  # Top 3 models\n",
    "    if model_name in credit_results:\n",
    "        y_proba = credit_results[model_name]['metrics']['probabilities']\n",
    "        fpr, tpr, _ = roc_curve(y_credit_test, y_proba)\n",
    "        auc_score = credit_results[model_name]['metrics']['roc_auc']\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Credit Data')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Confusion matrices for best models\n",
    "plt.subplot(2, 2, 3)\n",
    "best_fraud_predictions = fraud_results[best_fraud_model]['metrics']['predictions']\n",
    "cm_fraud = confusion_matrix(y_fraud_test, best_fraud_predictions)\n",
    "sns.heatmap(cm_fraud, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix - {best_fraud_model} (Fraud)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "best_credit_predictions = credit_results[best_credit_model]['metrics']['predictions']\n",
    "cm_credit = confusion_matrix(y_credit_test, best_credit_predictions)\n",
    "sns.heatmap(cm_credit, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix - {best_credit_model} (Credit)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest performing models:\")\n",
    "print(f\"Fraud Data: {best_fraud_model} (ROC AUC: {fraud_comparison.loc[fraud_comparison['Model'] == best_fraud_model, 'ROC_AUC'].values[0]:.4f})\")\n",
    "print(f\"Credit Data: {best_credit_model} (ROC AUC: {credit_comparison.loc[credit_comparison['Model'] == best_credit_model, 'ROC_AUC'].values[0]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Versioning and Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model versioning and registration\n",
    "print(\"=== MODEL VERSIONING ===\")\n",
    "\n",
    "# Register best models\n",
    "def register_best_model(model_name, dataset_name, results):\n",
    "    \"\"\"Register the best performing model\"\"\"\n",
    "    try:\n",
    "        # Get the latest run for this model\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        runs = mlflow.search_runs(\n",
    "            experiment_ids=[experiment.experiment_id],\n",
    "            filter_string=f\"params.model_name = '{model_name}' and params.dataset = '{dataset_name}'\",\n",
    "            order_by=[\"metrics.roc_auc DESC\"]\n",
    "        )\n",
    "        \n",
    "        if not runs.empty:\n",
    "            best_run = runs.iloc[0]\n",
    "            model_uri = f\"runs:/{best_run.run_id}/{model_name}_{dataset_name}\"\n",
    "            \n",
    "            # Register model\n",
    "            registered_model_name = f\"{model_name}_{dataset_name}_Production\"\n",
    "            mlflow.register_model(model_uri, registered_model_name)\n",
    "            print(f\"Registered model: {registered_model_name}\")\n",
    "            \n",
    "            return registered_model_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error registering model {model_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Register best models\n",
    "best_fraud_registered = register_best_model(best_fraud_model, \"Fraud\", fraud_results)\n",
    "best_credit_registered = register_best_model(best_credit_model, \"Credit\", credit_results)\n",
    "\n",
    "print(f\"\\nRegistered models:\")\n",
    "if best_fraud_registered:\n",
    "    print(f\"- {best_fraud_registered}\")\n",
    "if best_credit_registered:\n",
    "    print(f\"- {best_credit_registered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model artifacts and results\n",
    "print(\"=== SAVING MODEL ARTIFACTS ===\")\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save best traditional models\n",
    "best_fraud_traditional = fraud_comparison[~fraud_comparison['Model'].str.contains('Deep')].sort_values('ROC_AUC', ascending=False).iloc[0]['Model']\n",
    "best_credit_traditional = credit_comparison[~credit_comparison['Model'].str.contains('Deep')].sort_values('ROC_AUC', ascending=False).iloc[0]['Model']\n",
    "\n",
    "# Save fraud model\n",
    "if best_fraud_traditional in fraud_results:\n",
    "    joblib.dump(fraud_results[best_fraud_traditional]['model'], f'../models/best_fraud_model.pkl')\n",
    "    joblib.dump(scaler_fraud, f'../models/fraud_scaler.pkl')\n",
    "    print(f\"Saved best fraud model: {best_fraud_traditional}\")\n",
    "\n",
    "# Save credit model\n",
    "if best_credit_traditional in credit_results:\n",
    "    joblib.dump(credit_results[best_credit_traditional]['model'], f'../models/best_credit_model.pkl')\n",
    "    joblib.dump(scaler_credit, f'../models/credit_scaler.pkl')\n",
    "    print(f\"Saved best credit model: {best_credit_traditional}\")\n",
    "\n",
    "# Save deep learning models\n",
    "best_fraud_deep = fraud_comparison[fraud_comparison['Model'].str.contains('Deep')].sort_values('ROC_AUC', ascending=False)\n",
    "if not best_fraud_deep.empty:\n",
    "    best_fraud_deep_name = best_fraud_deep.iloc[0]['Model']\n",
    "    if best_fraud_deep_name in fraud_results:\n",
    "        fraud_results[best_fraud_deep_name]['model'].save('../models/best_fraud_deep_model.h5')\n",
    "        print(f\"Saved best fraud deep model: {best_fraud_deep_name}\")\n",
    "\n",
    "best_credit_deep = credit_comparison[credit_comparison['Model'].str.contains('Deep')].sort_values('ROC_AUC', ascending=False)\n",
    "if not best_credit_deep.empty:\n",
    "    best_credit_deep_name = best_credit_deep.iloc[0]['Model']\n",
    "    if best_credit_deep_name in credit_results:\n",
    "        credit_results[best_credit_deep_name]['model'].save('../models/best_credit_deep_model.h5')\n",
    "        print(f\"Saved best credit deep model: {best_credit_deep_name}\")\n",
    "\n",
    "# Save comparison results\n",
    "all_comparison.to_csv('../models/model_comparison_results.csv', index=False)\n",
    "print(\"\\nSaved model comparison results to CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed classification reports\n",
    "print(\"=== DETAILED CLASSIFICATION REPORTS ===\")\n",
    "\n",
    "print(f\"\\nBest Fraud Model ({best_fraud_traditional}) Classification Report:\")\n",
    "print(\"=\" * 60)\n",
    "fraud_pred = fraud_results[best_fraud_traditional]['metrics']['predictions']\n",
    "print(classification_report(y_fraud_test, fraud_pred, target_names=['Non-Fraud', 'Fraud']))\n",
    "\n",
    "print(f\"\\nBest Credit Model ({best_credit_traditional}) Classification Report:\")\n",
    "print(\"=\" * 60)\n",
    "credit_pred = credit_results[best_credit_traditional]['metrics']['predictions']\n",
    "print(classification_report(y_credit_test, credit_pred, target_names=['Non-Fraud', 'Fraud']))\n",
    "\n",
    "print(\"\\n=== TASK 2 COMPLETED SUCCESSFULLY ===\")\n",
    "print(\"All model building and training steps have been completed:\")\n",
    "print(\"✅ Data preparation and feature-target separation\")\n",
    "print(\"✅ Train-test split with stratification\")\n",
    "print(\"✅ MLOps setup with MLflow\")\n",
    "print(\"✅ Traditional ML models trained and evaluated\")\n",
    "print(\"✅ Deep learning models (MLP, CNN, LSTM) implemented\")\n",
    "print(\"✅ Model comparison and performance analysis\")\n",
    "print(\"✅ Model versioning and experiment tracking\")\n",
    "print(\"✅ Best models saved for deployment\")\n",
    "print(\"\\nReady for Task 3: Model Explainability!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3\n",
  },\n",
  "language_info": {\n",
   "codemirror_mode": {\n",
    "name": "ipython",\n",
    "version": 3\n",
   },\n",
   "file_extension": ".py",\n",
   "mimetype": "text/x-python",\n",
   "name": "python",\n",
   "nbconvert_exporter": "python",\n",
   "pygments_lexer": "ipython3",\n",
   "version": "3.8.5"\n",
  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}