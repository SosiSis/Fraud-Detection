name: Unit Tests and Code Quality

on:
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

jobs:
  lint-and-format:
    name: Code Quality Check
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install linting dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort mypy
    
    - name: Run Black (Code Formatting Check)
      run: |
        black --check --diff .
      continue-on-error: true
    
    - name: Run isort (Import Sorting Check)
      run: |
        isort --check-only --diff .
      continue-on-error: true
    
    - name: Run Flake8 (Linting)
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
      continue-on-error: true

  test-models-and-data:
    name: Test Data Processing and Models
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-xdist
    
    - name: Create necessary directories
      run: |
        mkdir -p models data logs explainability_results tests
    
    - name: Test data generation
      run: |
        python -c "
        import sys
        sys.path.append('.')
        try:
            exec(open('create_sample_data.py').read())
            print('‚úÖ Data generation successful')
        except Exception as e:
            print(f'‚ùå Data generation failed: {e}')
            sys.exit(1)
        "
    
    - name: Verify data files exist
      run: |
        ls -la data/
        test -f data/Fraud_Data.csv || (echo "‚ùå Fraud_Data.csv not found" && exit 1)
        test -f data/creditcard.csv || (echo "‚ùå creditcard.csv not found" && exit 1)
        test -f data/IpAddress_to_Country.csv || (echo "‚ùå IpAddress_to_Country.csv not found" && exit 1)
        echo "‚úÖ All data files exist"
    
    - name: Test model training
      run: |
        python -c "
        import sys
        sys.path.append('.')
        try:
            exec(open('train_models_for_api.py').read())
            print('‚úÖ Model training successful')
        except Exception as e:
            print(f'‚ùå Model training failed: {e}')
            sys.exit(1)
        "
    
    - name: Verify model files exist
      run: |
        ls -la models/
        test -f models/best_fraud_model.pkl || (echo "‚ùå fraud model not found" && exit 1)
        test -f models/best_credit_model.pkl || (echo "‚ùå credit model not found" && exit 1)
        echo "‚úÖ All model files exist"

  test-api-components:
    name: Test API Components
    runs-on: ubuntu-latest
    needs: test-models-and-data
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov
    
    - name: Create test environment
      run: |
        mkdir -p models data logs explainability_results
        
    - name: Generate test data and models
      run: |
        python create_sample_data.py
        python train_models_for_api.py
    
    - name: Test API module imports
      run: |
        python -c "
        try:
            import serve_model
            print('‚úÖ serve_model imports successfully')
        except Exception as e:
            print(f'‚ùå serve_model import failed: {e}')
            exit(1)
        "
    
    - name: Test Dashboard module imports
      run: |
        python -c "
        try:
            import dashboard_app
            print('‚úÖ dashboard_app imports successfully')
        except Exception as e:
            print(f'‚ùå dashboard_app import failed: {e}')
            exit(1)
        "
    
    - name: Test API model loading
      run: |
        python -c "
        import serve_model
        try:
            serve_model.load_models()
            assert serve_model.fraud_model is not None, 'Fraud model not loaded'
            assert serve_model.credit_model is not None, 'Credit model not loaded'
            print('‚úÖ API models loaded successfully')
        except Exception as e:
            print(f'‚ùå API model loading failed: {e}')
            exit(1)
        "
    
    - name: Test model predictions
      run: |
        python -c "
        import serve_model
        import numpy as np
        
        serve_model.load_models()
        
        # Test fraud prediction
        fraud_data = {
            'purchase_value': 100.0,
            'age': 35,
            'hour_of_day': 14,
            'day_of_week': 2,
            'source_encoded': 1,
            'browser_encoded': 0,
            'sex_encoded': 1
        }
        
        features = np.array([[fraud_data[field] for field in serve_model.FRAUD_FEATURES]])
        features_scaled = serve_model.fraud_scaler.transform(features)
        prediction = serve_model.fraud_model.predict(features_scaled)[0]
        probability = serve_model.fraud_model.predict_proba(features_scaled)[0]
        
        assert prediction in [0, 1], f'Invalid fraud prediction: {prediction}'
        assert len(probability) == 2, f'Invalid fraud probability shape: {len(probability)}'
        print('‚úÖ Fraud model prediction test passed')
        
        # Test credit prediction
        credit_data = {'Time': 12345.0, 'Amount': 100.0}
        for i in range(1, 29):
            credit_data[f'V{i}'] = 0.1 * i
        
        features = np.array([[credit_data[field] for field in serve_model.CREDIT_FEATURES]])
        features_scaled = serve_model.credit_scaler.transform(features)
        prediction = serve_model.credit_model.predict(features_scaled)[0]
        probability = serve_model.credit_model.predict_proba(features_scaled)[0]
        
        assert prediction in [0, 1], f'Invalid credit prediction: {prediction}'
        assert len(probability) == 2, f'Invalid credit probability shape: {len(probability)}'
        print('‚úÖ Credit model prediction test passed')
        "

  test-api-endpoints:
    name: Test API Endpoints
    runs-on: ubuntu-latest
    needs: test-models-and-data
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov
    
    - name: Create test environment
      run: |
        mkdir -p models data logs explainability_results
    
    - name: Generate test data and models
      run: |
        python create_sample_data.py
        python train_models_for_api.py
    
    - name: Start API server in background
      run: |
        export STARTUP_COMMAND=api
        export PORT=5000
        python serve_model.py &
        echo $! > api_pid.txt
        sleep 15  # Give the server more time to start and load models
      
    - name: Wait for API to be ready
      run: |
        timeout 60 bash -c 'until curl -f http://localhost:5000/health; do sleep 2; done'
    
    - name: Run API integration tests
      run: |
        export API_BASE_URL=http://localhost:5000
        python test_api.py
    
    - name: Stop API server
      run: |
        if [ -f api_pid.txt ]; then
          kill $(cat api_pid.txt) || true
          rm api_pid.txt
        fi

  test-notebooks:
    name: Test Jupyter Notebooks
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install nbconvert jupyter
    
    - name: Create test environment
      run: |
        mkdir -p models data logs explainability_results
    
    - name: Generate test data
      run: |
        python create_sample_data.py
    
    - name: Test notebook execution (EDA)
      run: |
        jupyter nbconvert --to notebook --execute notebooks/EDA.ipynb --output EDA_test.ipynb
        echo "‚úÖ EDA notebook executed successfully"
      continue-on-error: true
    
    - name: Test notebook execution (Model Training)  
      run: |
        jupyter nbconvert --to notebook --execute notebooks/model_training.ipynb --output model_training_test.ipynb
        echo "‚úÖ Model training notebook executed successfully"
      continue-on-error: true

  test-docker-build:
    name: Test Docker Build
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Test Docker build
      run: |
        docker build -t fraud-detection-test .
        echo "‚úÖ Docker build successful"
    
    - name: Test Docker container startup (API)
      run: |
        # Start container in background
        docker run -d -p 5000:5000 -e STARTUP_COMMAND=api -e PORT=5000 --name fraud-api-test fraud-detection-test
        
        # Wait for container to be ready
        sleep 45
        
        # Test health endpoint
        for i in {1..60}; do
          if curl -f http://localhost:5000/health; then
            echo "‚úÖ API container is healthy"
            break
          fi
          if [ $i -eq 60 ]; then
            echo "‚ùå API container health check failed"
            docker logs fraud-api-test
            exit 1
          fi
          sleep 2
        done
        
        # Clean up
        docker stop fraud-api-test
        docker rm fraud-api-test

  security-scan:
    name: Security Vulnerability Scan
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit
    
    - name: Run Safety (dependency vulnerability check)
      run: |
        pip install -r requirements.txt
        safety check --json --output safety-report.json || true
        safety check || echo "‚ö†Ô∏è Security vulnerabilities found in dependencies"
      continue-on-error: true
    
    - name: Run Bandit (code security analysis)
      run: |
        bandit -r . -f json -o bandit-report.json || true
        bandit -r . || echo "‚ö†Ô∏è Security issues found in code"
      continue-on-error: true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: |
          safety-report.json
          bandit-report.json

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [lint-and-format, test-models-and-data, test-api-components, test-api-endpoints, test-notebooks, test-docker-build, security-scan]
    if: always()
    
    steps:
    - name: Test Results Summary
      run: |
        echo "## üìä Test Results Summary"
        echo "| Test Suite | Status |"
        echo "|------------|--------|"
        echo "| Code Quality | ${{ needs.lint-and-format.result }} |"
        echo "| Models & Data | ${{ needs.test-models-and-data.result }} |"  
        echo "| API Components | ${{ needs.test-api-components.result }} |"
        echo "| API Endpoints | ${{ needs.test-api-endpoints.result }} |"
        echo "| Jupyter Notebooks | ${{ needs.test-notebooks.result }} |"
        echo "| Docker Build | ${{ needs.test-docker-build.result }} |"
        echo "| Security Scan | ${{ needs.security-scan.result }} |"
        
        # Check if any critical tests failed
        if [[ "${{ needs.test-models-and-data.result }}" == "failure" || 
              "${{ needs.test-api-components.result }}" == "failure" || 
              "${{ needs.test-api-endpoints.result }}" == "failure" ||
              "${{ needs.test-docker-build.result }}" == "failure" ]]; then
          echo "‚ùå Critical tests failed!"
          exit 1
        else
          echo "‚úÖ All critical tests passed!"
        fi
